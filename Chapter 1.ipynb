{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch. 1.1\n",
    "In a data matrix D, each row represents an entry / observation and each column represents an attribute <br/>\n",
    "$x_i$ represents the $i^{th}$ observation and $X_j$ represents the $j^{th}$ attribute <br/>\n",
    "n represents the number of transactions (rows / observations <br/>\n",
    "d represents the dimensionality of the data (number of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch. 1.2\n",
    "## Numeric Attributes\n",
    "* A numeric value that can be continuous or discrete. If the value is in {0,1} then it is binary. \n",
    "    * Interval-scaled: Attributes where the difference between two of these attributes are important\n",
    "        * i.e. temperature (20 degrees difference) \n",
    "    * Ratio-scaled: Attributes where the difference as well as the ratios are important\n",
    "        * i.e. Age 2 times the age of some other person.\n",
    "        \n",
    "## Categorical Attributes\n",
    "* An attribute that has a set-valued domain, discrete number of possibilities per attribute.\n",
    "    * Nominal: The attribute values are unordered and thus only equal comparisons matter.\n",
    "        * i.e. gender (all equal)\n",
    "    * Ordinal: The attribute values are ordered and thus >, <, = comparisons are allowed.\n",
    "        * i.e. size (small, medium, large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch. 1.3 Data: Algebraic and Geometric View\n",
    "* Each row $x_i = (x_{i1}, x_{i2}, ..., x_{id})$ can be transposed to become a column vector\n",
    "* The $j^{th}$ stnadard basis vector $e_j$ is the d-dimensional unit vector like $(0,....,1_{j},....0)$ such that the $j^{th}$ component is 1 and the remaining values are 0. That way we can retrieve the $j^{th}$ value of a vector\n",
    "* Any vector can be written as a linear combination of the standard basis vector. $x_i = (x_{i1}, x_{i2}, ..., x_{id}) = x_{i1} * e_1 + x_{i2} * e_2 + x_{i3} * e_3 ... + x_{id} * e_d = \\sum_{j=1}^{d}x_{ij}*e{j}$\n",
    "    * i.e. $x_1 = 5.9e_1 + 3.0e_2 + 4.2e_3 = 5.9 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + 3.0 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + 4.2 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5.9 \\\\ 3.0 \\\\ 4.2 \\end{bmatrix}$\n",
    "\n",
    "## ch. 1.3.1 Distance and Angle\n",
    "Let $a = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ ... \\\\ a_m \\end{bmatrix}$ and $b = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ ... \\\\ b_m \\end{bmatrix}$\n",
    "\n",
    "### Dot Product\n",
    "$a^{T}b = (a_1, a_2, ..., a_m) * \\begin{bmatrix} b_1 \\\\ b_2 \\\\ ... \\\\ b_m \\end{bmatrix} = \\sum_{i=1}^{m}a_i*b_i$\n",
    "### Length\n",
    "Euclidean length of a vector is defined as $||a|| = \\sqrt{a^{T}a} = \\sqrt{a_{1}^{2} + a_{2}^{2} + ... + a_{m}^{2}} = \\sqrt{\\sum_{i=1}^{m}a_{i}^{2}}$ <br/>\n",
    "The unit vector in the direction of **a** is given as $u = \\frac{a}{||a||}$ which is the definition of a normalized vector. \n",
    "#### Euclidean Norm\n",
    "$||a||_p = (|a_1|^p + |a_2|^p + ... + |a_m|^p) ^{1/p} = (\\sum_{i=1}^{m}|a_1|^p)^{\\frac{1}{p}}$\n",
    "### Distance\n",
    "Euclidean distance between a and b is defined as $\\delta(a,b) = ||a-b|| = \\sqrt{(a-b)^{T}(a-b)} = \\sqrt{\\sum_{i=1}^{m}(a_i-b_i)^2}$\n",
    "Thus the length of a vector is it's distance from the 0 vector, $\\delta(a,0)$ <br/>\n",
    "We can also write out the euclidean distance function as the same as the euclidean norm function for a-b, thus $\\delta(a, b) = ||a-b||_p$\n",
    "* i.e. Look at example 1.3 page 11.\n",
    "### Angle\n",
    "Find angle between two vectors a and b with: \n",
    "$cos(\\theta) = \\frac{a^T * b}{||a|| ||b||} = (\\frac{a}{||a||})^T(\\frac{b}{||b||})$ <br/>\n",
    "Due to the nature of this formula, the only values that $\\theta$ can take on is between 0 and 180, thus the only values of cos($\\theta$) are between -1 and 1.\n",
    "### Orthogonality\n",
    "Two vectors are orthagonal if the transpose of one vector * the other vector is 0, implying that the angle between the two vectors is 90 degrees.\n",
    "\n",
    "### Mean and total variance\n",
    "mean of a data matrix D is the mean of the row vectors, so the mean of all of an entry's attributes <br/>\n",
    "$mean(D) = \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i$ returns a vector of means, each value in the vector of means is the mean of the $i^{th}$ row in the matrix D. However, a more common version of the data matrix D, where each row represents an observation and each column represetns an attribute, can easily be mutated, transposed, such that each row represets an attributes and column represents an observation. Thus, the mean of this transposed data matrix is a vector of means of each attribute, since we perform the mean operation row-wise in the matrix.\n",
    "\n",
    "### Total Variance\n",
    "Total variance is the average squared distance of each point from the mean. (remember delta is the euclidean distance between two vectors / values). Since the mean is a vector of means of each attributes, and we again assume that the D data matrix row-wise represents attributes, we can find the total variance by finding the difference of each value in the attribute row squared \n",
    "$var(D) = \\frac{1}{n}\\sum_{i=1}^{n}\\delta(x_i, \\mu)^2$ <br/>\n",
    "The total variance can also be thought of as the difference between the average of the squared magnitude of the data points <br/>\n",
    "$\\frac{1}{n}(\\sum_{i=1}^{n}||x_i||^2) - ||\\mu||^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris['data']).rename({0:'sepal length', 1:'sepal width', 2:'petal length', 3:'petal width'}, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5223.849999999998"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sepal length'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6811222222222221"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance(lst):\n",
    "    diff = lst - np.mean(lst)\n",
    "    return(sum(diff ** 2) / len(lst))\n",
    "variance(df['sepal length'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6811222222222223"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df['sepal length'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18675066666666668"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df['sepal width'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.092424888888889"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df['petal length'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5785315555555555"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df['petal width'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8678728888888889"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(df['sepal width']) + np.var(df['sepal length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length    0.685694\n",
       "sepal width     0.188004\n",
       "petal length    3.113179\n",
       "petal width     0.582414\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8736975391498881"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.var()['sepal length'] + df.var()['sepal width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between using pandas variance method and using numpys. Numpys is more accurate and true to the arithmetic. If you reference example 1.4, the variance provided in the example is closer to the numpy version than the pandas version. \n",
    "\n",
    "### Centered Data Matrix\n",
    "If we want to create a vector of z-scores for a given attribute, can use the following method / formula. Given a data matrix D: <br/>\n",
    "$Z = D - 1 * \\mu ^ T = \\begin{bmatrix} x_1^T \\\\ x_2^T \\\\ ... \\\\ x_n^T \\end{bmatrix} - \\begin{bmatrix} \\mu^T \\\\ \\mu^T \\\\ ... \\\\ \\mu^T \\end{bmatrix} = \\begin{bmatrix} x_1^T - \\mu^T \\\\ x_2^T -\\mu^T \\\\ ... \\\\ x_n^T -\\mu^T \\end{bmatrix} = \\begin{bmatrix} z_1^T \\\\ z_2^T \\\\ ... \\\\ z_n^T \\end{bmatrix}$ <br/>\n",
    "where 1 is the n-dimensional vector (nx1) vector used to extend $\\mu$ so that it can be subtracted from every value in $x_i$ to get the corresponding z score.\n",
    "\n",
    "### Orthogonal Projection\n",
    "\n",
    "Purpose is to project a point or vector onto another vector. Let there exist two vectors, **a** and **b**, if I wanted to decompose vector b to fit on vector a, I would break down vector **b** into two parts, **a** parallel and orthogonal part. We can break down b to look like $b = b_{||} + b_{\\perp} = p + r$ where p = parallel to aand r = orthogonal to **a**. The vector **p** is called the orthogonal projection, or projection, of **b** on the vector of **a**, as the vector **b** was altered so that it would point in the same direction as **a**, this is commonly used to put one vector in terms of another vector, a form of normalization. <br/>\n",
    "\n",
    "Thus the vector **r** = **b** - **p** which gives the *perpendicular distance* between **b** and **a**, which is **OFTEN INTERPRETED AS THE RESIDUAL ERROR BETWEEN a AND b** <br/>\n",
    "\n",
    "Since the vector **p** is parallel to **a**, we can find some constant *c* such that **p** = c**a**. Thus: <br/>\n",
    "$r = b - p = b - ca$ since **p** and **r** are orthagonal, <br/>\n",
    "$\\textbf{p}^T\\textbf{r} = (c\\textbf{a}^T) (\\textbf{b} - c\\textbf{a}) = c\\textbf{a}^T\\textbf{b} - c^2\\textbf{a}^T\\textbf{a} = 0$ this implies that $c = \\frac{\\textbf{a}^T\\textbf{b}}{\\textbf{a}^T\\textbf{a}}$ **NOTE:** We can't just cancel out the a^T from the num and denom like in most fractions b/c they are used in vector multiplication to get some value, which means you can't just cancel out. Therefore, the projection of **b** on **a** is $p = b_{||} = c\\textbf{a} = (\\frac{\\textbf{a}^T\\textbf{b}}{\\textbf{a}^T\\textbf{a}}) \\textbf{a}$\n",
    "\n",
    "### Linear Independence and Dimensionality\n",
    "\n",
    "What are linear combinations? Linear combinations is performing arithmetic operations on a vector to manipulate it. It is often used to derive new attributes, which is important in feature extraction and dimensionality reduction. Given a set of vector (**v1**, **v2**, ..., **vn**) the linear combination is <br/> \n",
    "$c_1\\textbf{v}_1 + c_2\\textbf{v}_2 + ... + c_k\\textbf{v}_k$ <br/>\n",
    "The set of all possible linear combinations is called the *span*, denoted as span(**v**1, **v**2, ..., **v**k). \n",
    "\n",
    "#### Row and Column Space\n",
    "Given a data matrix D where columns are represented as $X_i$ and rows are represented as $x_i$, the *column space*, denoted as col(D), is the set of all linear combinations of the *d* column vectors.  <br/>\n",
    "\n",
    "$col(D) = span(X_1, X_2, ... X_d)$ <br/>\n",
    "\n",
    "The *row space* of D, row(D), is the set of all linear combinations of the n row vectors <br/>\n",
    "\n",
    "$row(D) = span(x_1, x_2, ... x_d)$ <br/>\n",
    "\n",
    "We can also say <br/>\n",
    "\n",
    "$row(D) = col(D^T)$ <br/>\n",
    "\n",
    "### Linear Independence\n",
    "\n",
    "If vectors are linearly dependent, then at least one vector in the vector set can be written as all other vector with linear combinations. Mathematically, by the definition of a linear combination, we would need two vectors in the set of vectors such that if multiplied by a constant for both then added, the result would be 0. Thus: <br/>\n",
    "\n",
    "$c_1\\textbf{v}_1 + c_2\\textbf{v}_2 + ... + c_k\\textbf{v}_k = 0$ where at least one of the constants is non-zero. <br/>\n",
    " \n",
    "However a set of vectors is linearly independent if and only if <br/>\n",
    "\n",
    "$c_1\\textbf{v}_1 + c_2\\textbf{v}_2 + ... + c_k\\textbf{v}_k = 0 implies that c_1 = c_2 = ... = c_k = 0$ <br/> \n",
    "\n",
    "Essentially a set of vectors is linearly independent if none of them can be written as a linear combination of other vectors in the set. \n",
    "\n",
    "### Dimension and Rank\n",
    "\n",
    "A *basis* for S is a set of vectors that are liearly independent and span S. A basis is a minimal spanning set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Data: Probabilistic View\n",
    "\n",
    "Each numeric attribute X is a random variable can be defined as a function that will assign a reul number, input, to an outcome. The domain of X, the input, is the sample space of the problem. There are two possible random variables:\n",
    "* discrete: only takes a finite number of values\n",
    "* continuous: can take any value in its range\n",
    "Look at example 1.6 on page 18 for an example of a discrete random variable. \n",
    "\n",
    "### Probability Mass Function\n",
    "* $f(x) = P(X = x)$ the function f gives the probability that the random variable X has the exact value x.\n",
    "We can assume that the probability only appears in discrete values in the range of X and 0 for all other values. This function also has to follow the rules $f(x) >= 0 \\sum_{x} f(x) = 1$\n",
    "* An example can be seen in ex 1.7 page 19 \n",
    "* Bernoulli distribution denotes the probability distribution of for a success (getting 1 with probability p) and (getting 0 with probability 1 - p).\n",
    "* For a bernoulli distribution, can use a binomial distribution as its probability mass function since there are multiple ways of picking x observations out of n sample size. \n",
    "\n",
    "### Probability Density Function\n",
    "* if X, the random variable, is continuous, then the probability for discrete values is 0, thus, we would use a probability density function and using an integral over a range to find the probability over a range of values.\n",
    "$ P(X\\in[a,b]) = \\int_{a}^{b}f(x)dx$ <br/>\n",
    "$ f(x) >= 0, x \\in R $ <br/>\n",
    "\n",
    "### Cumulative Distribution Function\n",
    "* Regardless of discrete or continuous, we can find the probability of observing a value at most some given value\n",
    "$ F(x) = P(X <= x)$ <br/>\n",
    "* If X is discrete, we use sommations\n",
    "$ F(x) = P(x <= x) = \\sum_{u<=x}f(u)$ <br/>\n",
    "* If X is continuous, we use integrals\n",
    "$ F(x) = P(X <= x) = \\int_{-\\infty}{x} f(u)du$\n",
    "\n",
    "## 1.4.1 Bivariate Random Variables\n",
    "* We can perform pairwise analysis with 2 random variables $X_1, X_2$ s.t. <br/>\n",
    "$ X = \\begin{bmatrix}X_1 \\\\ X_2 \\end{bmatrix} $\n",
    "\n",
    "### Joint Probability Mass Function\n",
    "* If X1 and X2 are discrete random variables then X has a joint probability mass function s.t.\n",
    "$ f(x) = f(x_1, x_2) = P(X_1 = x_1, X_2 = x_2) = P(X = x) $ \n",
    "* The sum of all possible probabilities of X1 and X2 have to equal to 1. \n",
    "\n",
    "### Joint Probability Density Function\n",
    "* The probability density function is just like the PDF for univariate, but just take into account two variables. For continuous random variables, use joint probability density functions to find the probability of variables within a certain range of values. \n",
    "$P(X \\in W) = P(X \\in ([x_1 - \\epsilon, x_1 + \\epsilon ], [x_2 - \\epsilon, x_2 + \\epsilon])) = \\int_{x_1 - \\epsilon}{x_1 + \\epsilon}\\int_{x_2 - \\epsilon}{x_2 + \\epsilon}f(x_1, x_2)$\n",
    "\n",
    "### Joint Cumulative Distribution Function\n",
    "$F(x) = F(x_1, x_2) = P(X_1 <= x_1, X_2 <= x_2) = P(X <= x)$\n",
    "\n",
    "### Statistical Independence\n",
    "* Two random variables are indpendent if P(X1, X2) = P(X1) * P(X2) for two events\n",
    "\n",
    "## 1.4.2 Multivariate Random Variable\n",
    "* Just like the other probability random variables, the multivariate random variable assigns a vector of real numbers in the given sample space to a probability. If the attributes of the random variable is not numeric, then the random variable will map each attribute to a value on the vector. If the random variable is discrete, then: <br/>\n",
    "$f(x_1, x_2, ..., x_n) = P(X_1=x_1, X_2=x_2, ..., X_n=x_n)$ <br/>\n",
    "* If the random variable is continuous, then: <br/>\n",
    "$P((X_1, X_2, ... X_n)\\in W) = \\int_{(x_1, x_2, ..., x_d)^T\\in W)}...\\int f(x)dx$\n",
    "\n",
    "## 1.4.3 Random Sample and Statistics\n",
    "* population: set or universe of all entities under study\n",
    "* statistic: a sample\n",
    "\n",
    "### univariate sample\n",
    "* Given a random variable X, if you took a random sample of size n from said random variable, then we can get *n independent and identically distributed* random variables (IID). All of the variables are statistically independent of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
